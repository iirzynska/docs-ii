---
title: Intel AMX accelerated Multicloud GitOps with Openshift AI
date: 2024-02-07
validated: false
summary: This is extension of Multicloud GitOps pattern with Red Hat Openshift AI component to show the value of using Intel AMX.
products:
- Red Hat OpenShift Container Platform
- Red Hat Advanced Cluster Management
- Red Hat Openshift AI
- OpenVINO Toolkit Operator
- 4th Gen Intel Xeon Scalable processors with Intel Advanced Matrix Extensions (Intel AMX)
industries:
- General
aliases: /multicloud-gitops-rhoai-amx/
# uncomment once this exists
# pattern_logo: multicloud-gitops.png
pattern_logo: amx-intel-ai.png
links:
  install: mcg-rhoai-amx-getting-started
  help: https://groups.google.com/g/hybrid-cloud-patterns
  bugs: https://github.com/validatedpatterns-sandbox/amx-accelerated-multicloud-gitops/issues
---

include::modules/comm-attributes.adoc[]

:toc:
:imagesdir: /images
:_content-type: CONCEPT

[id="about-multicloud-gitops-rhoai-amx-pattern"]
== About the {amx-rhoai-mcg-pattern}

Use case::

* Use a GitOps approach to manage hybrid and multi-cloud deployments across both public and private clouds.
* Enable cross-cluster governance and application lifecycle management.
* Accelerate AI operations and improve computational performance by using Intel Advanced Matrix Extensions together with Openshift AI operator.
* Securely manage secrets across the deployment.
+
[NOTE]
====
Based on the requirements of a specific implementation, certain details might differ. However, all validated patterns that are based on a portfolio architecture, generalize one or more successful deployments of a use case.
====

Background::
Organizations are aiming to develop, deploy, and operate applications on an open hybrid cloud in a stable, simple, and secure way. This hybrid strategy includes multi-cloud deployments where workloads might be running on multiple clusters and on multiple clouds, private or public.
This strategy requires an infrastructure-as-code approach: GitOps. GitOps uses Git repositories as a single source of truth to deliver infrastructure-as-code. Submitted code checks the continuous integration (CI) process, while the continuous delivery (CD) process checks and applies requirements for things like security, infrastructure-as-code, or any other boundaries set for the application framework. All changes to code are tracked, making updates easy while also providing version control should a rollback be needed.
Moreover, organizations are looking for solutions that increase efficiency and at the same time reduce costs, what is possible using *{intel-4th-gen-xeon-processors}* with a new build-in accelerator - *Intel Advanced Matrix Extensions*.

[id="about-solution"]
== About the solution

This architecture covers hybrid and multi-cloud management with GitOps as shown in following figure. At a high level this requires a management hub, for DevOps and GitOps, and infrastructure that extends to one or more managed clusters running on private or public clouds. The automated infrastructure-as-code approach can manage the versioning of components and deploy according to the infrastructure-as-code configuration.

Benefits of Hybrid Multicloud management with GitOps:

* Unify management across cloud environments.
* Dynamic infrastructure security.
* Infrastructural continuous delivery best practices.

//figure 1 originally
.Overview of the solution including the business drivers, management hub, and the clusters under management
image::multicloud-gitops-rhoai-amx/hybrid-multicloud-management-gitops-hl-arch.png[Multicloud Architecture]

//Add something about hardware running with Xeons
In the following figure, logically, this solution can be viewed as being composed of an automation component, unified management including secrets management, and the clusters under management, all running on top of a user-chosen mixture of on-premise data centers and public clouds.

//figure 2 originally
.Logical diagram of {intel-amx} accelerated hybrid multi-cloud management with GitOps
image::multicloud-gitops-rhoai-amx/rhoai-amx-logical-diagram.png[Logical Architecture]

[id="about-technology"]
== About the technology

The following technologies are used in this solution:

https://www.redhat.com/en/technologies/cloud-computing/openshift/try-it[Red Hat OpenShift Platform]::
An enterprise-ready Kubernetes container platform built for an open hybrid cloud strategy. It provides a consistent application platform to manage hybrid cloud, public cloud, and edge deployments. It delivers a complete application platform for both traditional and cloud-native applications, allowing them to run anywhere. OpenShift has a pre-configured, pre-installed, and self-updating monitoring stack that provides monitoring for core platform components. It also enables the use of external secret management systems, for example, HashiCorp Vault in this case, to securely add secrets into the OpenShift platform.

https://www.redhat.com/en/technologies/cloud-computing/openshift/try-it[Red Hat OpenShift GitOps]::
A declarative application continuous delivery tool for Kubernetes based on the ArgoCD project. Application definitions, configurations, and environments are declarative and version controlled in Git. It can automatically push the desired application state into a cluster, quickly find out if the application state is in sync with the desired state, and manage applications in multi-cluster environments.

https://www.redhat.com/en/technologies/management/advanced-cluster-management[Red Hat Advanced Cluster Management for Kubernetes]::
Controls clusters and applications from a single console, with built-in security policies. Extends the value of Red Hat OpenShift by deploying apps, managing multiple clusters, and enforcing policies across multiple clusters at scale.

https://www.redhat.com/en/technologies/management/ansible[Red Hat Ansible Automation Platform]::
Provides an enterprise framework for building and operating IT automation at scale across hybrid clouds including edge deployments. It enables users across an organization to create, share, and manage automation, from development and operations to security and network teams.

https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-ai[Red Hat Openshift AI]::
A flexible, scalable MLOps platform with tools to build, deploy, and manage AI-enabled applications. OpenShift AI (previously called Red Hat OpenShift Data Science) supports the full lifecycle of AI/ML experiments and models, on-premise and in the public cloud.

https://catalog.redhat.com/software/container-stacks/detail/60649a56209af65d24b7ca9e[OpenVINO Toolkit Operator]::
https://github.com/openvinotoolkit/operator[OpenVINO Toolkit Operator]::
The Operator includes OpenVINO™ Notebooks for development of AI optimized workloads and OpenVINO™ Model Server for deployment that enables AI inference execution at scale, and exposes AI models via gRPC and REST API interfaces.

https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/advanced-matrix-extensions/overview.html[Intel® Advanced Matrix Extensions]::
A new built-in accelerator that improves the performance of deep-learning training and inference on the CPU and is ideal for workloads like natural-language processing, recommendation systems and image recognition.

Hashicorp Vault::
Provides a secure centralized store for dynamic infrastructure and applications across clusters, including over low-trust networks between clouds and data centers.

This solution also uses a variety of _observability tools_ including the Prometheus monitoring and Grafana dashboard that are integrated with OpenShift as well as components of the Observatorium meta-project which includes Thanos and the Loki API.


[id="extension-of-mcg"]
== {amx-rhoai-mcg-pattern}


// RHODS pattern description
The basic {mcg-pattern} has been extended to highlight the *{intel-4th-gen-xeon-processors}* capabilities, offering developers a streamlined pathway to accelerate their workloads through the integration of cutting-edge *{intel-amx}*, fostering efficiency and performance optimization in AI workloads.

The basic pattern has been extended with two components: {rhoai} and OpenVINO Toolkit Operator. First one, Openshift AI, provides a powerful AI/ML platform for building AI-enabled applications, where data scientists and developers can collaborate to move from experiment to production in a consistent environment quickly. It provides Jupyter application with selection of notebook servers, that contain prepared environments with required support and optimizations (ie. CUDA, PyTorch, Tensorflow, HabanaAI, etc.). OpenVINO Toolkit Operator manages OpenVINO components in the Openshift enviroment. One of them is OpenVINO™ Model Server (OVMS), a scalable and high-performance solution for serving machine learning models optimized for Intel® architectures. The other component, that was used in the solution is Notebook resource. Notebook resource integrates JupyterHub from OpenShift AI with a container image that includes developer tools from the OpenVINO toolkit and enables selecting a defined image OpenVINO™ Toolkit from the Jupyter Spawner choice list.


// The basic pattern has been extended by the AI application named `amx-app`. It runs Deep Interest Evolution Network (DIEN) inference using the Intel-optimized TensorFlow and measures its accuracy. DIEN is a machine learning model used in the field of recommender systems, particularly in the domain of personalized content recommendation.

// Since the `amx-app` application must be running on the node with CPU supporting {intel-amx}, Node Feature Discovery Operator (NFD) is deployed as a part of this pattern.
// NFD manages the detection of hardware features and configuration in an OpenShift Container Platform cluster. It labels the nodes with hardware-specific information. The kernel detects {intel-amx} at run-time, so there is no need to enable and configure it separately.

// A deployment of `amx-app` was created based on instructions from link:https://github.com/IntelAI/models/blob/master/quickstart/recommendation/tensorflow/dien/inference/cpu/README_SPR_DEV_CAT.md[Model Zoo for Intel® Architecture repository - TF DIEN inference] and uses  link:https://hub.docker.com/layers/intel/recommendation/tf-spr-dien-inference/images/sha256-085c43d838197ae92db8a056da254506abd667951a3ae11e47da48f2f47cb92f?context=explore[intel/recommendation:tf-spr-dien-inference image].

// An `amx-app` use persistent volume claim to download and prepare dataset. When the dataset is ready, an application runs and measures the inference accuracy. By enabling ONEDNN verbose all the compiled instructions are shown in the logs. The appearance of the __avx512_core_amx_bf16__ flag confirms that {intel-amx} is used.

//figure 7 originally - change it!!!
.Logs from `amx-app` pod
image::multicloud-gitops-rhoai-amx/rhoai-amx-app-log.png[Logs from amx-app pod]

[id="next-steps_mcg-index"]
== Next steps

* link:mcg-rhoai-amx-getting-started[Deploy the management hub] using Helm.
* Add a managed cluster to link:mcg-rhoai-amx-managed-cluster[deploy the managed cluster piece using ACM].
